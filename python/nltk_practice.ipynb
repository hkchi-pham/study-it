{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkchi-pham/study-it/blob/main/python/nltk_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo2SfjfOtvUw",
        "outputId": "bf91784f-8478-4c18-e618-ef43862fcddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from NLTK) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from NLTK) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from NLTK) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from NLTK) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt NLTK\n",
        "!pip install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Tải bộ dữ liệu cần cho tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE5V8-myt0Oh",
        "outputId": "13addea2-1264-4792-f9e7-62453248ed03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải toàn bộ corpus trong NLTK\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ-l6WF0t20i",
        "outputId": "ca191bbf-fbff-4faf-f892-e69f43f07ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lấy tệp hamlet trong corpus gutenberg\n",
        "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "print(hamlet)"
      ],
      "metadata": {
        "id": "_Y49VYUf81Br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b5d94d-cb24-468d-fef1-a8a046b5c67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**"
      ],
      "metadata": {
        "id": "v_aF8Mzh6PXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Hamlet corpus\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "words = word_tokenize(hamlet_text)\n",
        "\n",
        "def is_word(s):\n",
        "    return bool(re.fullmatch(r\"[A-Za-z]+\", s)) #filter word_only\n",
        "\n",
        "words = [word for word in words if is_word(word)]\n",
        "print(words[:20])\n",
        "print(len(words))\n",
        "\n",
        "# Output: List of sentences after tokenization"
      ],
      "metadata": {
        "id": "w7Ru54WT6T7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587600a3-d607-4837-cc0e-a74d2738b252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', 'Actus', 'Primus', 'Scoena', 'Prima', 'Enter', 'Barnardo', 'and', 'Francisco', 'two', 'Centinels', 'Barnardo', 'Who', 's']\n",
            "30293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenization**"
      ],
      "metadata": {
        "id": "rjC8sxD08G21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Hamlet corpus\n",
        "from nltk.tokenize import sent_tokenize\n",
        "hamlet_text = \" \".join(hamlet)\n",
        "sentences = sent_tokenize(hamlet_text)\n",
        "print(sentences[:5])\n",
        "print(len(sentences))\n",
        "# Output: List of sentences after tokenization and total sentence count"
      ],
      "metadata": {
        "id": "D2H95ysW8Ied",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddf3abc-12b5-4a58-e720-7bbc48d745ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus .', 'Scoena Prima .', 'Enter Barnardo and Francisco two Centinels .', 'Barnardo .', \"Who ' s there ?\"]\n",
            "2365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "MSU73MbA8KTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Word list\n",
        "# Output: List of words after stemming and comparison with original words\n",
        "\n",
        "word_list = [\n",
        "    \"running\", \"flies\", \"cats\", \"children\", \"better\", \"easily\", \"happier\", \"beautifully\",\n",
        "    \"singing\", \"thinking\", \"faster\", \"studies\", \"studying\", \"builds\", \"builders\", \"bigger\",\n",
        "    \"strongest\", \"funniest\", \"doing\", \"swimming\", \"friends\", \"walking\", \"played\", \"eating\",\n",
        "    \"worked\", \"trying\", \"jumping\", \"flying\", \"houses\", \"mice\", \"babies\", \"prettier\",\n",
        "    \"softly\", \"quickest\", \"saddest\", \"important\", \"nicest\", \"dogs\", \"programming\",\n",
        "    \"machines\", \"learning\", \"teacher\", \"schools\", \"teaching\", \"quickly\", \"slowest\",\n",
        "    \"happiest\", \"dancing\", \"watching\", \"easiest\"\n",
        "]\n",
        "\n",
        "#PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "print(\"PorterStemmer\")\n",
        "for word in word_list:\n",
        "    print(word, \":\", PorterStemmer().stem(word))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iXmy1vF88Ltd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ff24f5-0522-4159-8c71-2e8e3953dc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer\n",
            "running : run\n",
            "flies : fli\n",
            "cats : cat\n",
            "children : children\n",
            "better : better\n",
            "easily : easili\n",
            "happier : happier\n",
            "beautifully : beauti\n",
            "singing : sing\n",
            "thinking : think\n",
            "faster : faster\n",
            "studies : studi\n",
            "studying : studi\n",
            "builds : build\n",
            "builders : builder\n",
            "bigger : bigger\n",
            "strongest : strongest\n",
            "funniest : funniest\n",
            "doing : do\n",
            "swimming : swim\n",
            "friends : friend\n",
            "walking : walk\n",
            "played : play\n",
            "eating : eat\n",
            "worked : work\n",
            "trying : tri\n",
            "jumping : jump\n",
            "flying : fli\n",
            "houses : hous\n",
            "mice : mice\n",
            "babies : babi\n",
            "prettier : prettier\n",
            "softly : softli\n",
            "quickest : quickest\n",
            "saddest : saddest\n",
            "important : import\n",
            "nicest : nicest\n",
            "dogs : dog\n",
            "programming : program\n",
            "machines : machin\n",
            "learning : learn\n",
            "teacher : teacher\n",
            "schools : school\n",
            "teaching : teach\n",
            "quickly : quickli\n",
            "slowest : slowest\n",
            "happiest : happiest\n",
            "dancing : danc\n",
            "watching : watch\n",
            "easiest : easiest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LancasterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "print(\"LancasterStemmer\")\n",
        "for word in word_list:\n",
        "    print(word,\":\", LancasterStemmer().stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHz2JXMhfSko",
        "outputId": "5a2f9be8-048d-4b29-d1bd-49ed19454b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LancasterStemmer\n",
            "running : run\n",
            "flies : fli\n",
            "cats : cat\n",
            "children : childr\n",
            "better : bet\n",
            "easily : easy\n",
            "happier : happy\n",
            "beautifully : beauty\n",
            "singing : sing\n",
            "thinking : think\n",
            "faster : fast\n",
            "studies : study\n",
            "studying : study\n",
            "builds : build\n",
            "builders : build\n",
            "bigger : big\n",
            "strongest : strongest\n",
            "funniest : funniest\n",
            "doing : doing\n",
            "swimming : swim\n",
            "friends : friend\n",
            "walking : walk\n",
            "played : play\n",
            "eating : eat\n",
            "worked : work\n",
            "trying : try\n",
            "jumping : jump\n",
            "flying : fly\n",
            "houses : hous\n",
            "mice : mic\n",
            "babies : baby\n",
            "prettier : pretty\n",
            "softly : soft\n",
            "quickest : quickest\n",
            "saddest : saddest\n",
            "important : import\n",
            "nicest : nicest\n",
            "dogs : dog\n",
            "programming : program\n",
            "machines : machin\n",
            "learning : learn\n",
            "teacher : teach\n",
            "schools : schools\n",
            "teaching : teach\n",
            "quickly : quick\n",
            "slowest : slowest\n",
            "happiest : happiest\n",
            "dancing : dant\n",
            "watching : watch\n",
            "easiest : easiest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "HI-54E0p8Sdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Word list\n",
        "# Output: Lemmatized words and comparison with original words\n",
        "\n",
        "word_list = [\n",
        "    \"running\", \"flies\", \"cats\", \"children\", \"better\", \"easily\", \"happier\", \"beautifully\",\n",
        "    \"singing\", \"thinking\", \"faster\", \"studies\", \"studying\", \"builds\", \"builders\", \"bigger\",\n",
        "    \"strongest\", \"funniest\", \"doing\", \"swimming\", \"friends\", \"walking\", \"played\", \"eating\",\n",
        "    \"worked\", \"trying\", \"jumping\", \"flying\", \"houses\", \"mice\", \"babies\", \"prettier\",\n",
        "    \"softly\", \"quickest\", \"saddest\", \"important\", \"nicest\", \"dogs\", \"programming\",\n",
        "    \"machines\", \"learning\", \"teacher\", \"schools\", \"teaching\", \"quickly\", \"slowest\",\n",
        "    \"happiest\", \"dancing\", \"watching\", \"easiest\"\n",
        "]\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatizer\")\n",
        "for word in word_list:\n",
        "    print(word,\":\", lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "id": "j1EjZQIv8Uuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40776c93-0e08-48ec-89c5-f2d8b6badf13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatizer\n",
            "running : running\n",
            "flies : fly\n",
            "cats : cat\n",
            "children : child\n",
            "better : better\n",
            "easily : easily\n",
            "happier : happier\n",
            "beautifully : beautifully\n",
            "singing : singing\n",
            "thinking : thinking\n",
            "faster : faster\n",
            "studies : study\n",
            "studying : studying\n",
            "builds : build\n",
            "builders : builder\n",
            "bigger : bigger\n",
            "strongest : strongest\n",
            "funniest : funniest\n",
            "doing : doing\n",
            "swimming : swimming\n",
            "friends : friend\n",
            "walking : walking\n",
            "played : played\n",
            "eating : eating\n",
            "worked : worked\n",
            "trying : trying\n",
            "jumping : jumping\n",
            "flying : flying\n",
            "houses : house\n",
            "mice : mouse\n",
            "babies : baby\n",
            "prettier : prettier\n",
            "softly : softly\n",
            "quickest : quickest\n",
            "saddest : saddest\n",
            "important : important\n",
            "nicest : nicest\n",
            "dogs : dog\n",
            "programming : programming\n",
            "machines : machine\n",
            "learning : learning\n",
            "teacher : teacher\n",
            "schools : school\n",
            "teaching : teaching\n",
            "quickly : quickly\n",
            "slowest : slowest\n",
            "happiest : happiest\n",
            "dancing : dancing\n",
            "watching : watching\n",
            "easiest : easiest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words**"
      ],
      "metadata": {
        "id": "kDR_BcUf8jBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Sample text (1000 sentences)\n",
        "# Output: Words after removing stop words and count of removed words\n",
        "\n",
        "text = \"\"\"\n",
        "The world we live in is full of opportunities, but it is also full of challenges. And while we often think about success, we rarely consider the effort required to achieve it. If you look around, you will see that there are people who work hard every single day. They do not stop, even when the odds are against them. It is clear that resilience is important. You might wonder why some succeed while others do not, but the answer is simple: persistence. For those who continue to try, even after failure, success often comes eventually. With time, patience, and effort, almost anything is possible. This is why so many people emphasize the importance of not giving up. When you face obstacles, it is easy to feel discouraged. But that is when you must remind yourself of your goals. In moments of doubt, think about why you started. That alone can be enough to keep you moving forward.\n",
        "\n",
        "It is also important to recognize that success is not a straight line. There are ups and downs, twists and turns, and moments of uncertainty. The journey is rarely ever smooth. But it is worth it. As you navigate life, you will encounter people who doubt you. They may say that you cannot do something, but you must not let them define your limits. Your potential is far greater than anyone else can imagine. And while it is good to listen to advice, you should also trust yourself. If you believe in yourself, you can accomplish more than you think. This is not just a motivational statement; it is a proven fact. The most successful people in history are those who refused to give up. By learning from their mistakes and continuing to push forward, they achieved greatness. At the heart of every success story is a person who chose to keep going, no matter how hard it got.\n",
        "\n",
        "Of course, not everything will go according to plan. Sometimes, you will need to adapt. And that is okay. On the path to success, flexibility is key. When things go wrong, take a moment to reassess and adjust. It is better to change your approach than to give up altogether. This kind of mindset will help you overcome any obstacle. With every challenge, there comes an opportunity to grow. For instance, failing at something can teach you valuable lessons. In fact, most people learn more from failure than from success. That is why failure should never be feared. You should embrace it as part of the process. As long as you keep trying, you will always have a chance to succeed. The only true failure is giving up.\n",
        "\n",
        "And yet, many people struggle with self-doubt. It is natural to question yourself sometimes. But remember, self-doubt is not a reason to stop. If anything, it is a sign that you are stepping out of your comfort zone. This is where growth happens. When you face your fears, you become stronger. They say courage is not the absence of fear, but the ability to act despite it. You do not need to be fearless; you just need to be brave enough to try. With every step forward, you build confidence. Your efforts, no matter how small, add up over time. And as you progress, you will see results. It may take longer than you expect, but that is okay. The important thing is to keep going.\n",
        "\n",
        "In addition to resilience, gratitude is also important. When you appreciate what you have, you create space for more positivity in your life. This does not mean you should settle for less, but rather that you should recognize the good things already present. They can serve as a reminder of how far you have come. By focusing on gratitude, you shift your mindset from scarcity to abundance. That shift can make all the difference. For example, instead of dwelling on what you lack, think about what you have achieved so far. As you do this, you will feel more motivated to keep going. The path to success is not just about achieving goals; it is also about enjoying the journey. With the right mindset, every step becomes meaningful.\n",
        "\n",
        "Of all the challenges you will face, fear of failure is perhaps the most common. But failure is not something to be afraid of. It is simply a part of life. You cannot succeed without taking risks, and risks often lead to failure. This is not a bad thing. When you fail, you learn. And when you learn, you grow. They say that every failure brings you one step closer to success. If you keep this in mind, you will see failure not as an end, but as a beginning. Your perspective matters more than anything else. With a positive outlook, even setbacks can become stepping stones. The key is to keep moving forward, no matter what happens.\n",
        "\n",
        "And while persistence is important, so is balance. It is essential to take care of yourself along the way. When you are tired, rest. If you are stressed, take a break. This does not mean you are giving up; it means you are recharging. In fact, taking care of yourself is one of the most productive things you can do. By prioritizing your well-being, you ensure that you have the energy and focus to achieve your goals. They say you cannot pour from an empty cup, and this is true. Your health and happiness are just as important as your ambitions.\n",
        "\n",
        "On the subject of happiness, it is worth noting that success is not the only source of joy. The little things in life matter just as much. As you work toward your goals, do not forget to enjoy the present moment. It is easy to get caught up in the future, but life is happening now. When you appreciate the present, you create memories that last a lifetime. And these memories are just as valuable as any achievement. There is beauty in the journey, not just the destination.\n",
        "\"\"\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_word = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "filter = [word for word in word_tokens if not word.lower() in stop_word]\n",
        "\n",
        "filter_word = [w for w in filter if is_word(w)]\n",
        "print(filter)\n",
        "print(f\"remove {len(text)-len(filter)} words\")\n",
        "print(filter_word[:100])"
      ],
      "metadata": {
        "id": "c1FI_qWU8kSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024d5998-d4d9-4d9f-b931-1155735c46b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['world', 'live', 'full', 'opportunities', ',', 'also', 'full', 'challenges', '.', 'often', 'think', 'success', ',', 'rarely', 'consider', 'effort', 'required', 'achieve', '.', 'look', 'around', ',', 'see', 'people', 'work', 'hard', 'every', 'single', 'day', '.', 'stop', ',', 'even', 'odds', '.', 'clear', 'resilience', 'important', '.', 'might', 'wonder', 'succeed', 'others', ',', 'answer', 'simple', ':', 'persistence', '.', 'continue', 'try', ',', 'even', 'failure', ',', 'success', 'often', 'comes', 'eventually', '.', 'time', ',', 'patience', ',', 'effort', ',', 'almost', 'anything', 'possible', '.', 'many', 'people', 'emphasize', 'importance', 'giving', '.', 'face', 'obstacles', ',', 'easy', 'feel', 'discouraged', '.', 'must', 'remind', 'goals', '.', 'moments', 'doubt', ',', 'think', 'started', '.', 'alone', 'enough', 'keep', 'moving', 'forward', '.', 'also', 'important', 'recognize', 'success', 'straight', 'line', '.', 'ups', 'downs', ',', 'twists', 'turns', ',', 'moments', 'uncertainty', '.', 'journey', 'rarely', 'ever', 'smooth', '.', 'worth', '.', 'navigate', 'life', ',', 'encounter', 'people', 'doubt', '.', 'may', 'say', 'something', ',', 'must', 'let', 'define', 'limits', '.', 'potential', 'far', 'greater', 'anyone', 'else', 'imagine', '.', 'good', 'listen', 'advice', ',', 'also', 'trust', '.', 'believe', ',', 'accomplish', 'think', '.', 'motivational', 'statement', ';', 'proven', 'fact', '.', 'successful', 'people', 'history', 'refused', 'give', '.', 'learning', 'mistakes', 'continuing', 'push', 'forward', ',', 'achieved', 'greatness', '.', 'heart', 'every', 'success', 'story', 'person', 'chose', 'keep', 'going', ',', 'matter', 'hard', 'got', '.', 'course', ',', 'everything', 'go', 'according', 'plan', '.', 'Sometimes', ',', 'need', 'adapt', '.', 'okay', '.', 'path', 'success', ',', 'flexibility', 'key', '.', 'things', 'go', 'wrong', ',', 'take', 'moment', 'reassess', 'adjust', '.', 'better', 'change', 'approach', 'give', 'altogether', '.', 'kind', 'mindset', 'help', 'overcome', 'obstacle', '.', 'every', 'challenge', ',', 'comes', 'opportunity', 'grow', '.', 'instance', ',', 'failing', 'something', 'teach', 'valuable', 'lessons', '.', 'fact', ',', 'people', 'learn', 'failure', 'success', '.', 'failure', 'never', 'feared', '.', 'embrace', 'part', 'process', '.', 'long', 'keep', 'trying', ',', 'always', 'chance', 'succeed', '.', 'true', 'failure', 'giving', '.', 'yet', ',', 'many', 'people', 'struggle', 'self-doubt', '.', 'natural', 'question', 'sometimes', '.', 'remember', ',', 'self-doubt', 'reason', 'stop', '.', 'anything', ',', 'sign', 'stepping', 'comfort', 'zone', '.', 'growth', 'happens', '.', 'face', 'fears', ',', 'become', 'stronger', '.', 'say', 'courage', 'absence', 'fear', ',', 'ability', 'act', 'despite', '.', 'need', 'fearless', ';', 'need', 'brave', 'enough', 'try', '.', 'every', 'step', 'forward', ',', 'build', 'confidence', '.', 'efforts', ',', 'matter', 'small', ',', 'add', 'time', '.', 'progress', ',', 'see', 'results', '.', 'may', 'take', 'longer', 'expect', ',', 'okay', '.', 'important', 'thing', 'keep', 'going', '.', 'addition', 'resilience', ',', 'gratitude', 'also', 'important', '.', 'appreciate', ',', 'create', 'space', 'positivity', 'life', '.', 'mean', 'settle', 'less', ',', 'rather', 'recognize', 'good', 'things', 'already', 'present', '.', 'serve', 'reminder', 'far', 'come', '.', 'focusing', 'gratitude', ',', 'shift', 'mindset', 'scarcity', 'abundance', '.', 'shift', 'make', 'difference', '.', 'example', ',', 'instead', 'dwelling', 'lack', ',', 'think', 'achieved', 'far', '.', ',', 'feel', 'motivated', 'keep', 'going', '.', 'path', 'success', 'achieving', 'goals', ';', 'also', 'enjoying', 'journey', '.', 'right', 'mindset', ',', 'every', 'step', 'becomes', 'meaningful', '.', 'challenges', 'face', ',', 'fear', 'failure', 'perhaps', 'common', '.', 'failure', 'something', 'afraid', '.', 'simply', 'part', 'life', '.', 'succeed', 'without', 'taking', 'risks', ',', 'risks', 'often', 'lead', 'failure', '.', 'bad', 'thing', '.', 'fail', ',', 'learn', '.', 'learn', ',', 'grow', '.', 'say', 'every', 'failure', 'brings', 'one', 'step', 'closer', 'success', '.', 'keep', 'mind', ',', 'see', 'failure', 'end', ',', 'beginning', '.', 'perspective', 'matters', 'anything', 'else', '.', 'positive', 'outlook', ',', 'even', 'setbacks', 'become', 'stepping', 'stones', '.', 'key', 'keep', 'moving', 'forward', ',', 'matter', 'happens', '.', 'persistence', 'important', ',', 'balance', '.', 'essential', 'take', 'care', 'along', 'way', '.', 'tired', ',', 'rest', '.', 'stressed', ',', 'take', 'break', '.', 'mean', 'giving', ';', 'means', 'recharging', '.', 'fact', ',', 'taking', 'care', 'one', 'productive', 'things', '.', 'prioritizing', 'well-being', ',', 'ensure', 'energy', 'focus', 'achieve', 'goals', '.', 'say', 'pour', 'empty', 'cup', ',', 'true', '.', 'health', 'happiness', 'important', 'ambitions', '.', 'subject', 'happiness', ',', 'worth', 'noting', 'success', 'source', 'joy', '.', 'little', 'things', 'life', 'matter', 'much', '.', 'work', 'toward', 'goals', ',', 'forget', 'enjoy', 'present', 'moment', '.', 'easy', 'get', 'caught', 'future', ',', 'life', 'happening', '.', 'appreciate', 'present', ',', 'create', 'memories', 'last', 'lifetime', '.', 'memories', 'valuable', 'achievement', '.', 'beauty', 'journey', ',', 'destination', '.']\n",
            "remove 4952 words\n",
            "['world', 'live', 'full', 'opportunities', 'also', 'full', 'challenges', 'often', 'think', 'success', 'rarely', 'consider', 'effort', 'required', 'achieve', 'look', 'around', 'see', 'people', 'work', 'hard', 'every', 'single', 'day', 'stop', 'even', 'odds', 'clear', 'resilience', 'important', 'might', 'wonder', 'succeed', 'others', 'answer', 'simple', 'persistence', 'continue', 'try', 'even', 'failure', 'success', 'often', 'comes', 'eventually', 'time', 'patience', 'effort', 'almost', 'anything', 'possible', 'many', 'people', 'emphasize', 'importance', 'giving', 'face', 'obstacles', 'easy', 'feel', 'discouraged', 'must', 'remind', 'goals', 'moments', 'doubt', 'think', 'started', 'alone', 'enough', 'keep', 'moving', 'forward', 'also', 'important', 'recognize', 'success', 'straight', 'line', 'ups', 'downs', 'twists', 'turns', 'moments', 'uncertainty', 'journey', 'rarely', 'ever', 'smooth', 'worth', 'navigate', 'life', 'encounter', 'people', 'doubt', 'may', 'say', 'something', 'must', 'let']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS Tagging**"
      ],
      "metadata": {
        "id": "gi9-RCHm8qZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Hamlet corpus\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "word = word_tokenize(hamlet_text)\n",
        "pos = pos_tag(word)\n",
        "print(pos[:20])\n",
        "\n",
        "select_tag = ['NN','VB','JJ']\n",
        "count_pos_tag = Counter(tag for word, tag in pos if tag in select_tag)\n",
        "print(count_pos_tag)\n",
        "# Output: POS tags for the first 20 words and count of specific tags (NN, VB, JJ)"
      ],
      "metadata": {
        "id": "rkyv3Y8M8sxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8289c10e-c274-4405-fd79-7dd88ac458ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('[', 'IN'), ('The', 'DT'), ('Tragedie', 'NNP'), ('of', 'IN'), ('Hamlet', 'NNP'), ('by', 'IN'), ('William', 'NNP'), ('Shakespeare', 'NNP'), ('1599', 'CD'), (']', 'NNP'), ('Actus', 'NNP'), ('Primus', 'NNP'), ('.', '.'), ('Scoena', 'NNP'), ('Prima', 'NNP'), ('.', '.'), ('Enter', 'NNP'), ('Barnardo', 'NNP'), ('and', 'CC'), ('Francisco', 'NNP')]\n",
            "Counter({'NN': 4206, 'JJ': 1691, 'VB': 1551})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "eDcLaTBu8uwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: Hamlet corpus\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "sent = sentences[:10]\n",
        "sent = \" \".join(sent)\n",
        "word = word_tokenize(sent)\n",
        "pos = pos_tag(word)\n",
        "ner = ne_chunk(pos)\n",
        "\n",
        "def classify(text):\n",
        "  label_list = {\n",
        "      'ORGANIZATION': 'Organization',\n",
        "      'PERSON': 'Person',\n",
        "      'LOCATION': 'Location'\n",
        "  }\n",
        "  for subtree in text.subtrees():\n",
        "\n",
        "      if subtree.label() in label_list:\n",
        "          entity = \"\"\n",
        "          for leaf in subtree.leaves():\n",
        "              entity = entity + leaf[0] + \" \"\n",
        "          print(subtree.label(), entity)\n",
        "\n",
        "classify(ner)\n",
        "# Output: Named entities from the first 10 sentences and their classification"
      ],
      "metadata": {
        "id": "JG3z2AJW8xTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927d05d7-6666-4c46-bced-ac3158473350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORGANIZATION Tragedie \n",
            "PERSON William Shakespeare \n",
            "PERSON Scoena Prima \n",
            "PERSON Enter Barnardo \n",
            "PERSON Barnardo \n",
            "PERSON Who \n",
            "PERSON Long \n",
            "PERSON Barnardo \n"
          ]
        }
      ]
    }
  ]
}